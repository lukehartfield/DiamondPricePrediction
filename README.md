# ğŸ’ Predicting Diamond Prices Using Machine Learning

*Data Science Project â€“ University of Texas at Austin*

---

### ğŸ“ Overview

This project explored how **physical characteristics and quality attributes** of diamonds (e.g., carat, cut, clarity, color, dimensions) influence pricing. Using a dataset of **53,940 diamonds**, we applied multiple machine learning models to predict price and uncover feature importance.

Our goal was twofold:

1. Understand which features most strongly influence diamond prices.
2. Build a highly accurate regression model to predict future prices.

---

### ğŸ§  Problem Framing

> Research Question: Can we accurately predict the price of a diamond using its physical and quality attributes?
> 

We framed this as a **supervised regression** task using historical data and evaluated multiple ML models based on **RMSE**, **RÂ²**, and residual analysis.

---

### ğŸ§ª Data Science Methods Used

| Category | Techniques Applied |
| --- | --- |
| **EDA** | Visualizations (pairplot, correlation matrix, bar charts) |
| **Feature Engineering** | Ordinal encoding (cut, color, clarity), calculated volume (length Ã— width Ã— depth) |
| **ML Models** | Linear Regression, Decision Tree, Pruned Tree, Bagging, Random Forest, XGBoost |
| **Evaluation Metrics** | RMSE, MSE, RÂ², Actual vs. Predicted Scatter Plots |
| **Validation Strategy** | 80/20 Train/Test Split |

---

### ğŸ“Š Modeling Results

| Model | RMSE | RÂ² |
| --- | --- | --- |
| âŒ Linear Regression | 1204.86 | 0.91 |
| âš ï¸ Decision Tree (Pruned) | 1333.14 | 0.89 |
| âœ… Random Forest | **535.14** | **0.98** |
| âœ… XGBoost | 542.84 | 0.88 |

> ğŸ“Œ Random Forest was the best-performing model, with 98% RÂ² and $535 average error in predictions.
> 

---

### ğŸ“ˆ Feature Insights

Using feature importance analysis, we discovered:

- **Carat and Volume** are the strongest predictors of price.
- **Clarity and Color** also significantly impact pricing.
- Surprisingly, **Cut**, **Depth**, and **Table Size** had minimal influence.

---

### ğŸ› ï¸ Key Skills Gained

| Category | Skills & Tools |
| --- | --- |
| **Data Prep** | Feature engineering, outlier handling, encoding categorical data |
| **Modeling** | Scikit-learn, XGBoost, tree-based ensemble methods |
| **Evaluation** | Interpreting RMSE/RÂ², error diagnostics, comparing model robustness |
| **ML Thinking** | Handling overfitting, using ensembling (bagging/boosting), pruning trees |
| **Team Collaboration** | Co-authoring analysis, discussing model tradeoffs, version control with Jupyter/IPython |

---

### ğŸ“Œ Takeaways

- Raw features (carat, clarity) don't tell the full storyâ€”*derived features* like volume add value.
- Simpler models (like linear regression) struggled due to nonlinear interactions and multicollinearity.
- **Tree-based models** (Random Forest & XGBoost) captured complex patterns and outperformed linear approaches by a wide margin.
- Even in high-dimensional but structured datasets, **model interpretability and domain logic** remain crucial.

---

### ğŸ’¡ Impact

This model could help:

- **Consumers** avoid overpaying for low-value stones
- **Sellers** price more competitively
- **Marketplaces** improve transparency and pricing consistency

The approach is generalizable to other luxury goods or any domain with **structured, numeric, and ordinal features** driving price.
